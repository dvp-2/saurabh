# -*- coding: utf-8 -*-
"""DQN_Agent_Arch2_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dKUbDV1zbEOQIJEanWn8wOUAHsOP3sUX

### Cab-Driver Agent
"""

#from google.colab import drive
#drive.mount('/content/drive')

#! cp /content/drive/MyDrive/Env.py .

#! cp /content/drive/MyDrive/TM.npy .

#! pip install keras

# Importing libraries
import numpy as np
import random
import math
from collections import deque
import collections
import pickle
import sys
import os

# for building DQN model
from tensorflow.keras import layers
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten, Input, BatchNormalization
from tensorflow.keras.optimizers import Adam

# for plotting graphs
import matplotlib.pyplot as plt

# Import the environment
from Env import CabDriver

"""#### Defining Time Matrix"""

# Loading the time matrix provided
Time_Matrix = np.load("TM.npy")

"""#### Tracking the state-action pairs for checking convergence

"""

def initialise_tracking_states():
    global States_Action_track 
    States_Action_track= dict()
    states = [(0,2,2),(0,13,5),(0,0,1),(0,14,6),(0,19,2),
              (1,0,0),(1,4,0),(1,5,3),(1,8,3),(1,3,5),
              (2,2,0),(2,5,2),(2,10,5),(2,20,4),(2,1,6),
              (3,2,1),(3,3,2),(3,22,4),(3,17,5),(3,6,6),
              (4,20,2),(4,12,4),(4,18,5),(4,16,2),(4,23,1)
             ]
    actions_ = [(i,j) for i in range(5) for j in range(5) if i!=j or j==0]
    for state in states:
        States_Action_track[state] = dict()
        actions = random.choices(actions_, k = 4)
        for action in actions:
            States_Action_track[state][action]=[]

def track_state_action():
    for state1, act_id, reward, next_state in agent.memory:
        for state in States_Action_track.keys():
            for action in States_Action_track[state].keys():
                action_id = env.action_space.index(action)
                if (state1 == state) and (act_id == action_id):
                    States_Action_track[state][action].append(reward)

#Defining a function to save the Q-dictionary as a pickle file
def save_obj(obj, name ):
    with open(name + '.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

"""### Agent Class

If you are using this framework, you need to fill the following to complete the following code block:
1. State and Action Size
2. Hyperparameters
3. Create a neural-network model in function 'build_model()'
4. Define epsilon-greedy strategy in function 'get_action()'
5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory
6. Complete the 'train_model()' function with following logic:
   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:
      - Initialise your input and output batch for training the model
      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))
      - Get Q(s', a) values from the last trained model
      - Update the input batch as your encoded state-action and output batch as your Q-values
      - Then fit your DQN model using the updated input and output batch.
"""

class DQNAgent:
    def __init__(self, state_size, action_size):
        # Define size of state and action
        self.state_size = state_size
        self.action_size = action_size

        # Write here: Specify you hyper parameters for the DQN
        self.discount_factor = 1e-02
        self.learning_rate = 1e-03      
        self.epsilon_max = 0.4
        self.epsilon_decay = 4e-04
        self.epsilon_min = 1e-05
        self.batch_size = 32        

        # create replay memory using deque
        self.memory = deque(maxlen=2000)

        # create main model and target model
        self.model = self.build_model()
        self.target_model = self.build_model()
        
        try:
            self.model.load_weights('cab.h5')
        except OSError:
            pass
        self.update_target_model()
        self.epsilon = self.epsilon_max
        self.model_loss = []

    
    
    
    # approximate Q function using Neural Network
    
    
    
    def build_model(self):
        
        model = Sequential()
        # Write your code here: Add layers to your neural nets 
        model.add(Input(self.state_size))
        model.add(Dense(36),activation='relu')
        model.add(BatchNormalization())
        model.add(Dense(68),activation = 'relu')
        #model.add(Dense(136))
        model.add(BatchNormalization())
        model.add(Dense(action_size))
        if episode == 0:
            model.summary()
        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))
        return model
    


    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())
        
    def get_action(self, state,requests,episode_num):
        # Write your code here:
        # get action from model using epsilon-greedy policy
        # Decay in Îµ after we generate each sample from the environment
        
        epsilon = self.epsilon_min + (self.epsilon_max-self.epsilon_min) * np.exp(-self.epsilon_decay * env.TIME)
        z = np.random.random()
        
        if self.epsilon_max>z:
            idx=random.choice(requests[0])
            return idx
        else:
            state_encoded = env.state_encod_arch2(state)
            state_encoded = np.array(state_encoded).reshape((1,36))
            q_value = self.model.predict(state_encoded)
            idxs = requests[0]
            Q_value = q_value[0]             
    
            return np.argmax(Q_value[idxs])
        
    

    def append_sample(self, state, id_action, reward, next_state):
        # Write your code here:
        # save sample <s,a,r,s'> to the replay memory
        self.memory.append((state, id_action, reward, next_state))
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    

    # pick samples randomly from replay memory (with batch_size) and train the network
    def train_model(self):
        if len(self.memory) > self.batch_size:
            # Sample batch from the memory
            mini_batch = random.sample(self.memory, self.batch_size)
            update_output = np.zeros((self.batch_size, self.state_size))     # write here
            update_input = np.zeros((self.batch_size, self.state_size)) # write here
            id_action, reward = [], []
            
            for i in range(self.batch_size):
                state_, id_action_, reward_, next_state_ = mini_batch[i]
                state_encod = env.state_encod_arch2(state_)
                update_input[i] = state_encod
                id_action.append(id_action_)
                reward.append(reward_)
                update_output[i] = env.state_encod_arch2(next_state_)
                
                
            # Write your code from here
            # 1. Predict the target from earlier model
            target = self.model.predict(update_input)
            target_val = self.target_model.predict(update_output)
                
            # 2. Get the target for the Q-network
            for i in range(self.batch_size):
                if terminal_state:
                    target[i][id_action[i]] = reward[i]
                else:
                    target[i][id_action[i]] = reward[i] + self.discount_factor * (np.amax(target_val[i]))
                
                
                #3. Update your 'update_output' and 'update_input' batch. Be careful to use the encoded state-action pair

                
                
        # 4. Fit your model and track the loss values            
            history=self.model.fit(update_input, target, batch_size = self.batch_size, epochs = 1)
            self.model_loss=history.history['loss']

    def get_loss(self):
        return self.model_loss
    
    def save(self, name):
        self.model.save_weights(name)

Episodes = 10000

"""### DQN block"""

loss = []

initialise_tracking_states()

env = CabDriver()
for episode in range(Episodes):
    
    # Write code here
    # Call the environment
    
    # Call all the initialised variables of the environment
    action_size = 21
    state_size = 36
    curr_state = env.state_init        
    terminal_state = False
    #Call the DQN agent 
    agent = DQNAgent(state_size,action_size)
    
    print(f'\nEPISODE:{episode}' )

    while not terminal_state:
        
        # Write your code here
        reqs = env.get_requests(curr_state)
        idx_curr_action = agent.get_action(curr_state,reqs,episode)
        loss+=agent.get_loss()

        # 2. Evaluate your reward and next state
        curr_reward = env.reward_func(curr_state,idx_curr_action,Time_Matrix)
        next_state = env.next_state_func(curr_state,idx_curr_action,Time_Matrix)
        
        # 3. Append the experience to the memory
        agent.append_sample(curr_state,idx_curr_action,curr_reward,next_state)
        
        # 4. Train the model by calling function agent.train_model
        agent.train_model()
        
        curr_state = next_state

        if env.TIME%24 == 0:
            print('..',end = '')
        if env.TIME//24 == 30:
            terminal_state = True

        # 5. Keep a track of rewards, Q-values, loss
        if terminal_state:
            agent.update_target_model()
            track_state_action()
            agent.save('cab.h5')

    env.reset()
            
save_obj(loss,'Loss')  
save_obj(States_Action_track, 'State-Action Pairs')

"""### Tracking Convergence"""

# loss = agent.get_loss()
plt.figure(figsize = (100,20))
plt.title(f'No. of Layers = 4, Episodes = {Episodes}')
plt.plot(loss)

#data = pickle.load('State-Action Pairs.pkl')
#for state in data.keys():





"""#### Epsilon-decay sample function

<div class="alert alert-block alert-info">
Try building a similar epsilon-decay function for your model.
</div>
"""

#time = np.arange(0,10000)
#epsilon = []
#for i in range(0,10000):
#    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))

#plt.plot(time, epsilon)
#plt.show()

